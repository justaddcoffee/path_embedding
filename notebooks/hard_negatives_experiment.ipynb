{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Negatives Experiment\n",
    "\n",
    "This notebook compares two negative sampling strategies:\n",
    "1. **Standard**: Shuffle all intermediate nodes (existing approach)\n",
    "2. **Hard**: Replace only one random intermediate node (new approach)\n",
    "\n",
    "The hypothesis is that hard negatives are more challenging and may help the model learn more robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Import path_embedding modules\n",
    "from path_embedding.data.drugmechdb import load_drugmechdb\n",
    "from path_embedding.utils.path_extraction import build_multigraph, extract_paths\n",
    "from path_embedding.data.negative_sampling import generate_negatives, generate_hard_negatives\n",
    "from path_embedding.embedding.openai_embedder import load_api_key, embed_paths\n",
    "from path_embedding.model.classifier import train_classifier\n",
    "from path_embedding.model.data_split import split_by_indication\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = \"../data/indication_paths.yaml\"\n",
    "API_KEY_PATH = \"/Users/jtr4v/openai.key.another\"\n",
    "\n",
    "# Parameters\n",
    "MAX_PATHS_PER_INDICATION = 10\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading DrugMechDB data...\")\n",
    "indications = load_drugmechdb(DATA_PATH)\n",
    "print(f\"Loaded {len(indications)} indications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting paths from multigraphs...\")\n",
    "all_positive_paths = []\n",
    "skipped_count = 0\n",
    "\n",
    "for indication in indications:\n",
    "    graph = build_multigraph(indication)\n",
    "    try:\n",
    "        paths = extract_paths(graph, indication[\"graph\"][\"_id\"], max_paths=MAX_PATHS_PER_INDICATION)\n",
    "        all_positive_paths.extend(paths)\n",
    "    except ValueError:\n",
    "        skipped_count += 1\n",
    "\n",
    "print(f\"Extracted {len(all_positive_paths)} positive paths (skipped {skipped_count} invalid indications)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting train/test by indication...\")\n",
    "train_pos, test_pos = split_by_indication(all_positive_paths, test_size=TEST_SIZE, random_seed=RANDOM_SEED)\n",
    "print(f\"Split: {len(train_pos)} train positive, {len(test_pos)} test positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment 1: Standard Negatives (Baseline)\n",
    "\n",
    "Generate negatives using the existing approach (shuffle all intermediate nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: STANDARD NEGATIVES (SHUFFLE ALL NODES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nGenerating standard negatives...\")\n",
    "train_neg_standard = generate_negatives(train_pos)\n",
    "test_neg_standard = generate_negatives(test_pos)\n",
    "print(f\"Generated {len(train_neg_standard)} train negatives, {len(test_neg_standard)} test negatives\")\n",
    "\n",
    "train_paths_standard = train_pos + train_neg_standard\n",
    "test_paths_standard = test_pos + test_neg_standard\n",
    "\n",
    "train_labels_standard = np.array([1] * len(train_pos) + [0] * len(train_neg_standard))\n",
    "test_labels_standard = np.array([1] * len(test_pos) + [0] * len(test_neg_standard))\n",
    "\n",
    "print(f\"Train: {len(train_paths_standard)} paths ({len(train_pos)} pos, {len(train_neg_standard)} neg)\")\n",
    "print(f\"Test: {len(test_paths_standard)} paths ({len(test_pos)} pos, {len(test_neg_standard)} neg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading API key...\")\n",
    "api_key = load_api_key(API_KEY_PATH)\n",
    "\n",
    "print(\"Generating embeddings for standard negatives (train)...\")\n",
    "train_embeddings_standard = embed_paths(train_paths_standard, api_key)\n",
    "print(f\"Train embeddings shape: {train_embeddings_standard.shape}\")\n",
    "\n",
    "print(\"Generating embeddings for standard negatives (test)...\")\n",
    "test_embeddings_standard = embed_paths(test_paths_standard, api_key)\n",
    "print(f\"Test embeddings shape: {test_embeddings_standard.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Random Forest classifier...\")\n",
    "rf_model_standard = train_classifier(train_embeddings_standard, train_labels_standard, random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "rf_predictions_standard = rf_model_standard.predict(test_embeddings_standard)\n",
    "rf_probabilities_standard = rf_model_standard.predict_proba(test_embeddings_standard)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "standard_results = {\n",
    "    'accuracy': accuracy_score(test_labels_standard, rf_predictions_standard),\n",
    "    'precision': precision_score(test_labels_standard, rf_predictions_standard),\n",
    "    'recall': recall_score(test_labels_standard, rf_predictions_standard),\n",
    "    'f1': f1_score(test_labels_standard, rf_predictions_standard),\n",
    "    'roc_auc': roc_auc_score(test_labels_standard, rf_probabilities_standard)\n",
    "}\n",
    "\n",
    "print(\"\\nStandard Negatives Results:\")\n",
    "for metric, value in standard_results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 2: Hard Negatives\n",
    "\n",
    "Generate negatives using the new approach (replace only one node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: HARD NEGATIVES (REPLACE ONE NODE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nGenerating hard negatives...\")\n",
    "train_neg_hard = generate_hard_negatives(train_pos)\n",
    "test_neg_hard = generate_hard_negatives(test_pos)\n",
    "print(f\"Generated {len(train_neg_hard)} train negatives, {len(test_neg_hard)} test negatives\")\n",
    "\n",
    "train_paths_hard = train_pos + train_neg_hard\n",
    "test_paths_hard = test_pos + test_neg_hard\n",
    "\n",
    "train_labels_hard = np.array([1] * len(train_pos) + [0] * len(train_neg_hard))\n",
    "test_labels_hard = np.array([1] * len(test_pos) + [0] * len(test_neg_hard))\n",
    "\n",
    "print(f\"Train: {len(train_paths_hard)} paths ({len(train_pos)} pos, {len(train_neg_hard)} neg)\")\n",
    "print(f\"Test: {len(test_paths_hard)} paths ({len(test_pos)} pos, {len(test_neg_hard)} neg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating embeddings for hard negatives (train)...\")\n",
    "train_embeddings_hard = embed_paths(train_paths_hard, api_key)\n",
    "print(f\"Train embeddings shape: {train_embeddings_hard.shape}\")\n",
    "\n",
    "print(\"Generating embeddings for hard negatives (test)...\")\n",
    "test_embeddings_hard = embed_paths(test_paths_hard, api_key)\n",
    "print(f\"Test embeddings shape: {test_embeddings_hard.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Random Forest classifier...\")\n",
    "rf_model_hard = train_classifier(train_embeddings_hard, train_labels_hard, random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "rf_predictions_hard = rf_model_hard.predict(test_embeddings_hard)\n",
    "rf_probabilities_hard = rf_model_hard.predict_proba(test_embeddings_hard)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "hard_results = {\n",
    "    'accuracy': accuracy_score(test_labels_hard, rf_predictions_hard),\n",
    "    'precision': precision_score(test_labels_hard, rf_predictions_hard),\n",
    "    'recall': recall_score(test_labels_hard, rf_predictions_hard),\n",
    "    'f1': f1_score(test_labels_hard, rf_predictions_hard),\n",
    "    'roc_auc': roc_auc_score(test_labels_hard, rf_probabilities_hard)\n",
    "}\n",
    "\n",
    "print(\"\\nHard Negatives Results:\")\n",
    "for metric, value in hard_results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Standard Negatives': standard_results,\n",
    "    'Hard Negatives': hard_results\n",
    "})\n",
    "\n",
    "comparison_df['Difference'] = comparison_df['Standard Negatives'] - comparison_df['Hard Negatives']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: STANDARD VS HARD NEGATIVES\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.round(4))\n",
    "print(\"\\nNote: Positive difference means standard negatives are easier to classify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "comparison_df[['Standard Negatives', 'Hard Negatives']].T.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance: Standard vs Hard Negatives')\n",
    "ax.set_xticklabels(['Standard Negatives', 'Hard Negatives'], rotation=0)\n",
    "ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.set_ylim([0.6, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difficulty difference\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "metrics = list(standard_results.keys())\n",
    "differences = [comparison_df.loc[m, 'Difference'] for m in metrics]\n",
    "colors = ['green' if d > 0 else 'red' for d in differences]\n",
    "\n",
    "ax.bar(metrics, differences, color=colors, alpha=0.7)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.set_ylabel('Difference (Standard - Hard)')\n",
    "ax.set_title('Performance Difference\\n(Positive = Hard negatives are harder)')\n",
    "ax.set_xlabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Standard negatives errors\n",
    "standard_errors = test_labels_standard != rf_predictions_standard\n",
    "standard_false_positives = (test_labels_standard == 0) & (rf_predictions_standard == 1)\n",
    "standard_false_negatives = (test_labels_standard == 1) & (rf_predictions_standard == 0)\n",
    "\n",
    "print(\"\\nStandard Negatives:\")\n",
    "print(f\"  Total errors: {standard_errors.sum()} / {len(test_labels_standard)} ({standard_errors.sum()/len(test_labels_standard)*100:.2f}%)\")\n",
    "print(f\"  False positives: {standard_false_positives.sum()}\")\n",
    "print(f\"  False negatives: {standard_false_negatives.sum()}\")\n",
    "\n",
    "# Hard negatives errors\n",
    "hard_errors = test_labels_hard != rf_predictions_hard\n",
    "hard_false_positives = (test_labels_hard == 0) & (rf_predictions_hard == 1)\n",
    "hard_false_negatives = (test_labels_hard == 1) & (rf_predictions_hard == 0)\n",
    "\n",
    "print(\"\\nHard Negatives:\")\n",
    "print(f\"  Total errors: {hard_errors.sum()} / {len(test_labels_hard)} ({hard_errors.sum()/len(test_labels_hard)*100:.2f}%)\")\n",
    "print(f\"  False positives: {hard_false_positives.sum()}\")\n",
    "print(f\"  False negatives: {hard_false_negatives.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prediction confidence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Standard negatives\n",
    "axes[0].hist(rf_probabilities_standard[test_labels_standard == 1], bins=30, alpha=0.5, label='Positive', color='green')\n",
    "axes[0].hist(rf_probabilities_standard[test_labels_standard == 0], bins=30, alpha=0.5, label='Negative', color='red')\n",
    "axes[0].set_xlabel('Predicted Probability (Positive Class)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Standard Negatives: Prediction Confidence')\n",
    "axes[0].legend()\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Hard negatives\n",
    "axes[1].hist(rf_probabilities_hard[test_labels_hard == 1], bins=30, alpha=0.5, label='Positive', color='green')\n",
    "axes[1].hist(rf_probabilities_hard[test_labels_hard == 0], bins=30, alpha=0.5, label='Negative', color='red')\n",
    "axes[1].set_xlabel('Predicted Probability (Positive Class)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Hard Negatives: Prediction Confidence')\n",
    "axes[1].legend()\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from path_embedding.embedding.text_formatter import path_to_text\n",
    "\n",
    "# Show example of standard vs hard negative for same positive path\n",
    "example_idx = 0\n",
    "positive_example = train_pos[example_idx]\n",
    "standard_negative_example = train_neg_standard[example_idx]\n",
    "hard_negative_example = train_neg_hard[example_idx]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE: POSITIVE VS NEGATIVES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nPositive Path:\")\n",
    "print(path_to_text(positive_example)[:300] + \"...\")\n",
    "\n",
    "print(\"\\nStandard Negative (all nodes shuffled):\")\n",
    "print(path_to_text(standard_negative_example)[:300] + \"...\")\n",
    "\n",
    "print(\"\\nHard Negative (one node replaced):\")\n",
    "print(path_to_text(hard_negative_example)[:300] + \"...\")\n",
    "\n",
    "# Count node differences\n",
    "standard_diffs = sum(1 for i in range(len(positive_example.nodes)) \n",
    "                     if positive_example.nodes[i].id != standard_negative_example.nodes[i].id)\n",
    "hard_diffs = sum(1 for i in range(len(positive_example.nodes)) \n",
    "                 if positive_example.nodes[i].id != hard_negative_example.nodes[i].id)\n",
    "\n",
    "print(\"\\nNode differences from positive:\")\n",
    "print(f\"  Standard negative: {standard_diffs} nodes different\")\n",
    "print(f\"  Hard negative: {hard_diffs} nodes different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "acc_diff = standard_results['accuracy'] - hard_results['accuracy']\n",
    "\n",
    "print(\"\\n1. DIFFICULTY\")\n",
    "if acc_diff > 0.05:\n",
    "    print(f\"   Hard negatives are SIGNIFICANTLY harder to classify ({acc_diff:+.2%} accuracy drop)\")\n",
    "elif acc_diff > 0.02:\n",
    "    print(f\"   Hard negatives are MODERATELY harder to classify ({acc_diff:+.2%} accuracy drop)\")\n",
    "else:\n",
    "    print(f\"   Hard negatives have SIMILAR difficulty ({acc_diff:+.2%} accuracy change)\")\n",
    "\n",
    "print(\"\\n2. PERFORMANCE\")\n",
    "print(f\"   Standard Negatives: {standard_results['accuracy']:.2%} accuracy\")\n",
    "print(f\"   Hard Negatives: {hard_results['accuracy']:.2%} accuracy\")\n",
    "\n",
    "print(\"\\n3. RECOMMENDATION\")\n",
    "if hard_results['accuracy'] > 0.85:\n",
    "    print(\"   Hard negatives still achieve good performance (>85% accuracy)\")\n",
    "    print(\"   Consider using for more robust model training\")\n",
    "elif hard_results['accuracy'] > 0.75:\n",
    "    print(\"   Hard negatives achieve reasonable performance (75-85% accuracy)\")\n",
    "    print(\"   May be useful for specific applications requiring robustness\")\n",
    "else:\n",
    "    print(\"   Hard negatives may be too difficult (<75% accuracy)\")\n",
    "    print(\"   Standard negatives recommended for this task\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS\")\n",
    "print(\"   • Analyze which types of paths are harder to classify\")\n",
    "print(\"   • Try intermediate difficulty (replace 2-3 nodes)\")\n",
    "print(\"   • Use hard negatives as augmentation with standard negatives\")\n",
    "print(\"   • Analyze feature importance differences between models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
